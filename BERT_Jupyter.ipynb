{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tX9nDQnr8AzT"
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/6/6d/Nvidia_image_logo.svg\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# BERT Question Answering in TensorFlow with Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kL-6-WT78AzR"
   },
   "source": [
    "Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "FOa47jxd80bS",
    "outputId": "4c1db5fb-0dd8-45b3-d00b-bccfb1941e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Loy_jvmr8AzT"
   },
   "source": [
    "## 1. Overview\n",
    "\n",
    "Bidirectional Embedding Representations from Transformers (BERT), is a method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. \n",
    "\n",
    "The original paper can be found here: https://arxiv.org/abs/1810.04805.\n",
    "\n",
    "NVIDIA's BERT is an optimized version of Google's official implementation, leveraging mixed precision arithmetic and tensor cores on V100 GPUS for faster training times while maintaining target accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXp2mMCx8AzU"
   },
   "source": [
    "### Learning objectives\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Inference on Question Answering (QA) task with BERT Large model\n",
    "- The use/download of fine-tuned NVIDIA BERT models from [NGC](https://ngc.nvidia.com)\n",
    "- Use of Mixed Precision models for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xLlJiTQN8AzV"
   },
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiQ5qvJD8Azm"
   },
   "source": [
    "### Pre-Trained NVIDIA BERT TensorFlow Models on NGC\n",
    "\n",
    "<img src=\"https://blogs.nvidia.com/wp-content/uploads/2019/03/18-ngc-software-stack-447x500.png\" style=\"width: 360px;\">\n",
    "\n",
    "We will be using the following configuration of BERT in this example:\n",
    "\n",
    "| **Model** | **Hidden layers** | **Hidden unit size** | **Attention heads** | **Feedforward filter size** | **Max sequence length** | **Parameters** |\n",
    "|:---------:|:----------:|:----:|:---:|:--------:|:---:|:----:|\n",
    "|BERTLARGE|24 encoder|1024| 16|4 x 1024|512|330M|\n",
    "\n",
    "**To do so, we will take advantage of the pre-trained models available on the [NGC Model Registry](https://ngc.nvidia.com/catalog/models).**\n",
    "\n",
    "Among the many configurations available we will download one of these two:\n",
    "\n",
    " - **bert_tf_ckpt_large_qa_squad2_amp_384**\n",
    "\n",
    "which are trained on the [SQuaD 2.0 Dataset](https://rajpurkar.github.io/SQuAD-explorer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iJR47XD8Azg"
   },
   "source": [
    "We can choose the mixed precision model (which takes much less time to train than the fp32 version) without losing accuracy, with the following flag: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wT8mFmG51eUt"
   },
   "outputs": [],
   "source": [
    "use_mixed_precision_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /workspace/bert/data/finetuned_large_model_SQUAD2.0: Read-only file system\n",
      "/bin/bash: wget: command not found\n"
     ]
    }
   ],
   "source": [
    "# bert_tf_ckpt_large_qa_squad2_amp_384\n",
    "DATA_DIR_FT = '/workspace/bert/data/finetuned_large_model_SQUAD2.0'\n",
    "!mkdir -p $DATA_DIR_FT\n",
    "    \n",
    "!wget --content-disposition -O $DATA_DIR_FT/bert_tf_ckpt_large_qa_squad2_amp_384_19.03.1.zip  \\\n",
    "https://api.ngc.nvidia.com/v2/models/nvidia/bert_tf_ckpt_large_qa_squad2_amp_384/versions/19.03.1/zip \\\n",
    "&& unzip -n -d $DATA_DIR_FT/ $DATA_DIR_FT/bert_tf_ckpt_large_qa_squad2_amp_384_19.03.1.zip \\\n",
    "&& rm -rf $DATA_DIR_FT/bert_tf_ckpt_large_qa_squad2_amp_384_19.03.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuE_gCBUp6uD"
   },
   "source": [
    "### NGC Model Scripts\n",
    "\n",
    "While we're at it, we'll also pull down some BERT helper scripts from the [NGC Model Scripts Registry](https://ngc.nvidia.com/catalog/model-scripts/nvidia:bert_for_tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kavDaBXpqd7T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘bert_scripts.zip’ already there; not retrieving.\n",
      "Archive:  bert_scripts.zip\n",
      "   creating: /workspace/bert/bert_scripts/\n",
      "  inflating: /workspace/bert/__MACOSX/._bert_scripts  \n",
      "  inflating: /workspace/bert/bert_scripts/run_classifier_with_tfhub.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._run_classifier_with_tfhub.py  \n",
      "  inflating: /workspace/bert/bert_scripts/modeling_test.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._modeling_test.py  \n",
      "  inflating: /workspace/bert/bert_scripts/.DS_Store  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._.DS_Store  \n",
      "  inflating: /workspace/bert/bert_scripts/LICENSE  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._LICENSE  \n",
      "  inflating: /workspace/bert/bert_scripts/requirements.txt  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._requirements.txt  \n",
      "  inflating: /workspace/bert/bert_scripts/Dockerfile  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._Dockerfile  \n",
      "  inflating: /workspace/bert/bert_scripts/extract_features.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._extract_features.py  \n",
      "  inflating: /workspace/bert/bert_scripts/optimization_test.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._optimization_test.py  \n",
      "  inflating: /workspace/bert/bert_scripts/optimization.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._optimization.py  \n",
      "  inflating: /workspace/bert/bert_scripts/tokenization_test.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._tokenization_test.py  \n",
      "  inflating: /workspace/bert/bert_scripts/run_squad.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._run_squad.py  \n",
      "  inflating: /workspace/bert/bert_scripts/__init__.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/.___init__.py  \n",
      "  inflating: /workspace/bert/bert_scripts/tokenization.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._tokenization.py  \n",
      "  inflating: /workspace/bert/bert_scripts/run_pretraining.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._run_pretraining.py  \n",
      "  inflating: /workspace/bert/bert_scripts/run_classifier.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._run_classifier.py  \n",
      "  inflating: /workspace/bert/bert_scripts/NOTICE  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._NOTICE  \n",
      "  inflating: /workspace/bert/bert_scripts/README.md  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._README.md  \n",
      "  inflating: /workspace/bert/bert_scripts/sample_text.txt  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._sample_text.txt  \n",
      "  inflating: /workspace/bert/bert_scripts/.dockerignore  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._.dockerignore  \n",
      "  inflating: /workspace/bert/bert_scripts/CONTRIBUTING.md  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._CONTRIBUTING.md  \n",
      "  inflating: /workspace/bert/bert_scripts/gpu_environment.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._gpu_environment.py  \n",
      "  inflating: /workspace/bert/bert_scripts/create_pretraining_data.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._create_pretraining_data.py  \n",
      "   creating: /workspace/bert/bert_scripts/scripts/\n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._scripts  \n",
      "  inflating: /workspace/bert/bert_scripts/multilingual.md  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._multilingual.md  \n",
      "  inflating: /workspace/bert/bert_scripts/modeling.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._modeling.py  \n",
      "  inflating: /workspace/bert/bert_scripts/predicting_movie_reviews_with_bert_on_tf_hub.ipynb  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._predicting_movie_reviews_with_bert_on_tf_hub.ipynb  \n",
      "   creating: /workspace/bert/bert_scripts/data/\n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/._data  \n",
      "   creating: /workspace/bert/bert_scripts/scripts/docker/\n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._docker  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/data_download.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._data_download.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/finetune_train_benchmark.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._finetune_train_benchmark.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/run.sub  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._run.sub  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/start_pretraining.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._start_pretraining.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/data_download_helper.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._data_download_helper.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/finetune_inference_benchmark.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._finetune_inference_benchmark.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/run_squad_inference.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._run_squad_inference.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/run_squad.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._run_squad.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/run_pretraining.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/._run_pretraining.sh  \n",
      "   creating: /workspace/bert/bert_scripts/data/pretrained_models_google/\n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/._pretrained_models_google  \n",
      "   creating: /workspace/bert/bert_scripts/data/bookcorpus/\n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/._bookcorpus  \n",
      "  inflating: /workspace/bert/bert_scripts/data/README.md  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/._README.md  \n",
      "   creating: /workspace/bert/bert_scripts/data/squad/\n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/._squad  \n",
      "   creating: /workspace/bert/bert_scripts/data/wikipedia_corpus/\n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/._wikipedia_corpus  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/docker/launch.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/docker/._launch.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/scripts/docker/build.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/scripts/docker/._build.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/pretrained_models_google/download_models.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/pretrained_models_google/._download_models.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/create_pseudo_test_set.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._create_pseudo_test_set.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/preprocessing_test_set.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._preprocessing_test_set.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/preprocessing.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._preprocessing.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/sentence_segmentation_nltk.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._sentence_segmentation_nltk.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/preprocessing_test_set_xargs_wrapper.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._preprocessing_test_set_xargs_wrapper.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/create_pseudo_test_set.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._create_pseudo_test_set.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/shard_text_input_file.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._shard_text_input_file.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/config.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._config.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/run_preprocessing.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._run_preprocessing.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/preprocessing_xargs_wrapper.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._preprocessing_xargs_wrapper.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/bookcorpus/clean_and_merge_text.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/bookcorpus/._clean_and_merge_text.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/squad/squad_download.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/squad/._squad_download.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/create_pseudo_test_set.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._create_pseudo_test_set.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/remove_tags_and_clean.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._remove_tags_and_clean.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/preprocessing_test_set.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._preprocessing_test_set.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/preprocessing.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._preprocessing.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/preprocessing_test_set_xargs_wrapper.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._preprocessing_test_set_xargs_wrapper.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/wiki_sentence_segmentation_spacy_pipe.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._wiki_sentence_segmentation_spacy_pipe.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/wiki_sentence_segmentation_spacy.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._wiki_sentence_segmentation_spacy.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/create_pseudo_test_set.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._create_pseudo_test_set.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/wiki_sentence_segmentation_nltk.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._wiki_sentence_segmentation_nltk.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/shard_text_input_file.py  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._shard_text_input_file.py  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/config.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._config.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/run_preprocessing.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._run_preprocessing.sh  \n",
      "  inflating: /workspace/bert/bert_scripts/data/wikipedia_corpus/preprocessing_xargs_wrapper.sh  \n",
      "  inflating: /workspace/bert/__MACOSX/bert_scripts/data/wikipedia_corpus/._preprocessing_xargs_wrapper.sh  \n"
     ]
    }
   ],
   "source": [
    "# Download BERT helper scripts\n",
    "!wget -nc --show-progress -O bert_scripts.zip \\\n",
    "     https://api.ngc.nvidia.com/v2/recipes/nvidia/bert_for_tensorflow/versions/1/zip\n",
    "!mkdir -p /workspace/bert\n",
    "!unzip -n -d /workspace/bert bert_scripts.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEs0P1C_RPIi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-18 16:52:31--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.224.104\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.224.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: ‘/workspace/bert/config.qa/vocab.txt’\n",
      "\n",
      "/workspace/bert/con 100%[===================>] 226.08K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-05-18 16:52:32 (2.00 MB/s) - ‘/workspace/bert/config.qa/vocab.txt’ saved [231508/231508]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download BERT vocab file\n",
    "!mkdir -p /workspace/bert/config.qa\n",
    "!wget -nc https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt \\\n",
    "    -O /workspace/bert/config.qa/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO2tAJ5TRRUB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /workspace/bert/config.qa/bert_config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspace/bert/config.qa/bert_config.json\n",
    "{\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 1024,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 4096,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"num_attention_heads\": 16,\n",
    "  \"num_hidden_layers\": 24,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 30522\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dynamic JSON files based on user inputs\n",
    "def write_input_file(context, qinputs, predict_file):\n",
    "    # Remove quotes and new lines from text for valid JSON\n",
    "    context = context.replace('\"', '').replace('\\n', '')\n",
    "    # Create JSON dict to write\n",
    "    json_dict = {\n",
    "      \"data\": [\n",
    "        {\n",
    "          \"title\": \"BERT QA\",\n",
    "          \"paragraphs\": [\n",
    "            {\n",
    "              \"context\": context,\n",
    "              \"qas\": qinputs\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    # Write JSON to input file\n",
    "    with open(predict_file, 'w') as json_file:\n",
    "        import json\n",
    "        json.dump(json_dict, json_file, indent=2)\n",
    "    \n",
    "# Display Inference Results as HTML Table\n",
    "def display_results(predict_file, output_prediction_file):\n",
    "    import json\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    # Here we show only the prediction results, nbest prediction is also available in the output directory\n",
    "    results = \"\"\n",
    "    with open(predict_file, 'r') as query_file:\n",
    "        queries = json.load(query_file)\n",
    "        input_data = queries[\"data\"]\n",
    "        with open(output_prediction_file, 'r') as result_file:\n",
    "            data = json.load(result_file)\n",
    "            for entry in input_data:\n",
    "                for paragraph in entry[\"paragraphs\"]:\n",
    "                    for qa in paragraph[\"qas\"]:\n",
    "                        results += \"<tr><td>{}</td><td>{}</td><td>{}</td></tr>\".format(qa[\"id\"], qa[\"question\"], data[qa[\"id\"]])\n",
    "\n",
    "    display(HTML(\"<table><tr><th>Id</th><th>Question</th><th>Answer</th></tr>{}</table>\".format(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLdBPppf8AzV"
   },
   "source": [
    "## 3. BERT Inference: Question Answering\n",
    "\n",
    "We can run inference on a fine-tuned BERT model for tasks like Question Answering.\n",
    "\n",
    "Here we use a BERT model fine-tuned on a [SQuaD 2.0 Dataset](https://rajpurkar.github.io/SQuAD-explorer/) which contains 100,000+ question-answer pairs on 500+ articles combined with over 50,000 new, unanswerable questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-jHuLNk8AzW"
   },
   "source": [
    "### Paragraph and Queries\n",
    "\n",
    "In this example we will ask our BERT model questions related to the following paragraph:\n",
    "\n",
    "**The Apollo Program**\n",
    "_\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of landing a man on the Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\"_\n",
    "\n",
    "  \n",
    "---\n",
    "\n",
    "The paragraph and the questions can be easily customized by changing the code below:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dr_eMAtfSN5R"
   },
   "outputs": [],
   "source": [
    "# Create BERT input file with (1) context and (2) questions to be answered based on that context\n",
    "predict_file = '/workspace/bert/config.qa/input.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LcOfv3dn8AzX",
    "outputId": "ae803153-071a-4d5e-82df-8f684c9d0ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /workspace/bert/config.qa/input.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $predict_file\n",
    "{\"data\": \n",
    " [\n",
    "     {\"title\": \"Project Apollo\",\n",
    "      \"paragraphs\": [\n",
    "          {\"context\":\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of landing a man on the Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, and was supported by the two man Gemini program which ran concurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\", \n",
    "           \"qas\": [\n",
    "               { \"question\": \"What project put the first Americans into space?\", \n",
    "                 \"id\": \"Q1\"\n",
    "               },\n",
    "               { \"question\": \"What program was created to carry out these projects and missions?\",\n",
    "                 \"id\": \"Q2\"\n",
    "               },\n",
    "               { \"question\": \"What year did the first manned Apollo flight occur?\",\n",
    "                 \"id\": \"Q3\"\n",
    "               },                \n",
    "               { \"question\": \"What President is credited with the notion of putting Americans on the moon?\",\n",
    "                 \"id\": \"Q4\"\n",
    "               },\n",
    "               { \"question\": \"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\",\n",
    "                 \"id\": \"Q5\"\n",
    "               },\n",
    "               { \"question\": \"How long did Project Apollo run?\",\n",
    "                 \"id\": \"Q6\"\n",
    "               },               \n",
    "               { \"question\": \"What program helped develop space travel techniques that Project Apollo used?\",\n",
    "                 \"id\": \"Q7\"\n",
    "               },                \n",
    "               {\"question\": \"What space station supported three manned missions in 1973-1974?\",\n",
    "                 \"id\": \"Q8\"\n",
    "               }\n",
    "]}]}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNPDdF_f8Azq"
   },
   "source": [
    "## 4. Running Question/Answer Inference\n",
    "\n",
    "To run QA inference we will launch the script run_squad.py with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNA4ezvR8Azr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This specifies the model architecture.\n",
    "bert_config_file = '/workspace/bert/config.qa/bert_config.json'\n",
    "\n",
    "# The vocabulary file that the BERT model was trained on.\n",
    "vocab_file = '/workspace/bert/config.qa/vocab.txt'\n",
    "\n",
    "# Initiate checkpoint to the fine-tuned BERT Large model\n",
    "init_checkpoint = os.path.join('/workspace/bert/data/finetuned_large_model_SQUAD2.0/model.ckpt')\n",
    "\n",
    "# Create the output directory where all the results are saved.\n",
    "output_dir = '/workspace/bert/results'\n",
    "output_prediction_file = os.path.join(output_dir,'predictions.json')\n",
    "    \n",
    "# Whether to lower case the input - True for uncased models / False for cased models.\n",
    "do_lower_case = True\n",
    "  \n",
    "# Total batch size for predictions\n",
    "predict_batch_size = 8\n",
    "\n",
    "# Whether to run eval on the dev set.\n",
    "do_predict = True\n",
    "\n",
    "# When splitting up a long document into chunks, how much stride to take between chunks.\n",
    "doc_stride = 128\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization.\n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "max_seq_length = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcC81ooQ8Azt",
    "tags": []
   },
   "source": [
    "### 4a. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "No3_W3fd8Azt",
    "outputId": "552205fd-4e3e-48a8-898b-836412062d1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/workspace/bert/run_squad.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Ask BERT questions\n",
    "!python /workspace/bert/run_squad.py \\\n",
    "  --bert_config_file=$bert_config_file \\\n",
    "  --vocab_file=$vocab_file \\\n",
    "  --init_checkpoint=$init_checkpoint \\\n",
    "  --output_dir=$output_dir \\\n",
    "  --do_predict=$do_predict \\\n",
    "  --predict_file=$predict_file \\\n",
    "  --predict_batch_size=$predict_batch_size \\\n",
    "  --doc_stride=$doc_stride \\\n",
    "  --max_seq_length=$max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELf0wtQ08Azw"
   },
   "source": [
    "### 4b. Display Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZ0OZclQ8Azw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Id</th><th>Question</th><th>Answer</th></tr><tr><td>Q1</td><td>What project put the first Americans into space?</td><td>Project Mercury</td></tr><tr><td>Q2</td><td>What program was created to carry out these projects and missions?</td><td>The Apollo program</td></tr><tr><td>Q3</td><td>What year did the first manned Apollo flight occur?</td><td>1968</td></tr><tr><td>Q4</td><td>What President is credited with the notion of putting Americans on the moon?</td><td>John F. Kennedy</td></tr><tr><td>Q5</td><td>Who did the U.S. collaborate with on an Earth orbit mission in 1975?</td><td>Soviet Union</td></tr><tr><td>Q6</td><td>How long did Project Apollo run?</td><td>1961 to 1972</td></tr><tr><td>Q7</td><td>What program helped develop space travel techniques that Project Apollo used?</td><td>Gemini missions</td></tr><tr><td>Q8</td><td>What space station supported three manned missions in 1973-1974?</td><td>Skylab</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results(predict_file, output_prediction_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NH0Umn_e6Jsz"
   },
   "source": [
    "<details>\n",
    "  <summary><b>Click to reveal expected answers to the questions above</b></summary>\n",
    "  \n",
    "| Id | Question | Answer |\n",
    "|----|----------|--------|\n",
    "| Q1 | What project put the first Americans into space? | Project Mercury |\n",
    "| Q2 | What program was created to carry out these projects and missions? | The Apollo program |\n",
    "| Q3 | What year did the first manned Apollo flight occur? | 1968 |\n",
    "| Q4 | What President is credited with the notion of putting Americans on the moon?\t | John F. Kennedy |\n",
    "| Q5 | Who did the U.S. collaborate with on an Earth orbit mission in 1975? | Soviet Union |\n",
    "| Q6 | How long did Project Apollo run? | 1961 to 1972 |\n",
    "| Q7 | What program helped develop space travel techniques that Project Apollo used? | Gemini missions |\n",
    "| Q8 | What space station supported three manned missions in 1973-1974? | Skylab |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8EfbPm8Azz"
   },
   "source": [
    "## 5. Custom Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHWl7yus8Azz"
   },
   "source": [
    "Now that you are familiar with running QA Inference on BERT, you may want to try\n",
    "your own paragraphs and queries.\n",
    "\n",
    "\n",
    "1. Copy and paste your context from Wikipedia, news articles, etc. when prompted below\n",
    "2. Enter questions based on the context when prompted below.\n",
    "3. Run the inference script\n",
    "4. Display the inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvnB1JUpWV_a"
   },
   "outputs": [],
   "source": [
    "predict_file = '/workspace/bert/config.qa/custom_input.json'\n",
    "num_questions = 3           # You can configure this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryd1akIpBaKz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste your context here: Alan Mathison Turing OBE FRS (/ˈtjʊərɪŋ/; 23 June 1912 – 7 June 1954) was an English[6] mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.[7] Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[8][9][10] Turing is widely considered to be the father of theoretical computer science and artificial intelligence.[11] Despite these accomplishments, he was not fully recognised in his home country during his lifetime, due to his homosexuality, and because much of his work was covered by the Official Secrets Act.  During the Second World War, Turing worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bombe method, an electromechanical machine that could find settings for the Enigma machine.  Turing played a pivotal role in cracking intercepted coded messages that enabled the Allies to defeat the Nazis in many crucial engagements, including the Battle of the Atlantic, and in so doing helped win the war.[12][13] Due to the problems of counterfactual history, it's hard to estimate what effect Ultra intelligence had on the war,[14] but at the upper end it has been estimated that this work shortened the war in Europe by more than two years and saved over 14 million lives.[12]  After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, which was one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers[15] and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis[1] and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, first observed in the 1960s.  Turing was prosecuted in 1952 for homosexual acts; the Labouchere Amendment of 1885 had mandated that \"gross indecency\" was a criminal offence in the UK. He accepted chemical castration treatment, with DES, as an alternative to prison. Turing died in 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest determined his death as a suicide, but it has been noted that the known evidence is also consistent with accidental poisoning.  In 2009, following an Internet campaign, British Prime Minister Gordon Brown made an official public apology on behalf of the British government for \"the appalling way he was treated\". Queen Elizabeth II granted Turing a posthumous pardon in 2013. The Alan Turing law is now an informal term for a 2017 law in the United Kingdom that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.[16]  On 15 July 2019 the Bank of England announced that Turing would be depicted on the United Kingdom's new £50 note.\n"
     ]
    }
   ],
   "source": [
    "# Create your own context to ask questions about.\n",
    "context = input(\"Paste your context here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEalqfQXnZDT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1/3: Who is Alan Turing?\n",
      "Question 2/3: When was Alan Turing born?\n",
      "Question 3/3: Which laboratory Turing joined in 1948?\n"
     ]
    }
   ],
   "source": [
    "# Get questions from user input\n",
    "questions = [input(\"Question {}/{}: \".format(i+1, num_questions)) for i in range(num_questions)]\n",
    "# Format questions and write to JSON input file\n",
    "qinputs = [{ \"question\":q, \"id\":\"Q{}\".format(i+1)} for i,q in enumerate(questions)]\n",
    "write_input_file(context, qinputs, predict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_RbzPEGWeWE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-16 00:25:20.406731: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "WARNING:tensorflow:From /workspace/bert/optimization.py:110: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:162: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:1409: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:1174: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0316 00:25:21.868111 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:1174: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:1174: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "W0316 00:25:21.868280 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:1174: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/modeling.py:94: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0316 00:25:21.868470 140608677435200 module_wrapper.py:137] From /workspace/bert/modeling.py:94: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:1183: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "W0316 00:25:21.869253 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:1183: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:1199: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0316 00:25:21.998484 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:1199: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0316 00:25:21.998684 140608677435200 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fe139ce63a0>) includes params argument, but params are not passed to Estimator.\n",
      "W0316 00:25:22.744968 140608677435200 estimator.py:1992] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fe139ce63a0>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/workspace/bert/results', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe1302d2370>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "I0316 00:25:22.745511 140608677435200 estimator.py:212] Using config: {'_model_dir': '/workspace/bert/results', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': , '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe1302d2370>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "I0316 00:25:22.745804 140608677435200 tpu_context.py:220] _TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "W0316 00:25:22.745965 140608677435200 tpu_context.py:222] eval_on_tpu ignored because use_tpu is False.\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:266: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0316 00:25:22.746137 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:266: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:1112: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "W0316 00:25:22.749643 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:1112: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:1354: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W0316 00:25:22.830683 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:1354: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:***** Running predictions *****\n",
      "I0316 00:25:22.830802 140608677435200 run_squad.py:1354] ***** Running predictions *****\n",
      "INFO:tensorflow:  Num orig examples = 3\n",
      "I0316 00:25:22.830897 140608677435200 run_squad.py:1355]   Num orig examples = 3\n",
      "INFO:tensorflow:  Num split examples = 12\n",
      "I0316 00:25:22.831071 140608677435200 run_squad.py:1356]   Num split examples = 12\n",
      "INFO:tensorflow:  Batch size = 8\n",
      "I0316 00:25:22.831164 140608677435200 run_squad.py:1357]   Batch size = 8\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:731: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "W0316 00:25:22.831306 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:731: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "INFO:tensorflow:Could not find trained model in model_dir: /workspace/bert/results, running initialization to predict.\n",
      "I0316 00:25:22.831603 140608677435200 estimator.py:614] Could not find trained model in model_dir: /workspace/bert/results, running initialization to predict.\n",
      "WARNING:tensorflow:Entity <function input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7fe1302d1a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: 'arguments' object has no attribute 'defaults'\n",
      "W0316 00:25:22.888306 140608677435200 ag_logging.py:146] Entity <function input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7fe1302d1a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: 'arguments' object has no attribute 'defaults'\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:743: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "W0316 00:25:22.888502 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:743: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I0316 00:25:22.904676 140608677435200 estimator.py:1148] Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "I0316 00:25:22.904836 140608677435200 tpu_estimator.py:3124] Running infer on CPU\n",
      "WARNING:tensorflow:From /workspace/bert/modeling.py:175: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0316 00:25:22.907857 140608677435200 module_wrapper.py:137] From /workspace/bert/modeling.py:175: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/modeling.py:413: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0316 00:25:22.909193 140608677435200 module_wrapper.py:137] From /workspace/bert/modeling.py:413: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/modeling.py:494: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "W0316 00:25:22.935186 140608677435200 module_wrapper.py:137] From /workspace/bert/modeling.py:494: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:655: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "W0316 00:25:26.434307 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:655: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/bert/run_squad.py:670: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "W0316 00:25:26.440835 140608677435200 module_wrapper.py:137] From /workspace/bert/run_squad.py:670: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I0316 00:25:27.519404 140608677435200 estimator.py:1150] Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0316 00:25:28.309463 140608677435200 monitored_session.py:240] Graph was finalized.\n",
      "2021-03-16 00:25:28.339586: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2198615000 Hz\n",
      "2021-03-16 00:25:28.341293: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5978d70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-03-16 00:25:28.341321: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-03-16 00:25:28.344109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-03-16 00:25:28.580121: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8b42f70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-03-16 00:25:28.580169: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-DGXS-32GB, Compute Capability 7.0\n",
      "2021-03-16 00:25:28.583618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1665] Found device 0 with properties: \n",
      "name: Tesla V100-DGXS-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:08:00.0\n",
      "2021-03-16 00:25:28.583675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-03-16 00:25:28.589475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-03-16 00:25:28.591388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-03-16 00:25:28.591844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-03-16 00:25:28.596971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-03-16 00:25:28.597815: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-03-16 00:25:28.597983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-03-16 00:25:28.601731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1793] Adding visible gpu devices: 0\n",
      "2021-03-16 00:25:28.601770: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-03-16 00:25:28.994633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-03-16 00:25:28.994679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212]      0 \n",
      "2021-03-16 00:25:28.994692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1225] 0:   N \n",
      "2021-03-16 00:25:28.998624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1351] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 31026 MB memory) -> physical GPU (device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0)\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0316 00:25:31.745624 140608677435200 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0316 00:25:31.858720 140608677435200 session_manager.py:502] Done running local_init_op.\n",
      "2021-03-16 00:25:33.689415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "INFO:tensorflow:Processing example: 0\n",
      "I0316 00:25:34.384404 140608677435200 run_squad.py:1373] Processing example: 0\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I0316 00:25:34.586190 140608677435200 error_handling.py:101] prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I0316 00:25:34.586374 140608677435200 error_handling.py:101] prediction_loop marked as finished\n",
      "INFO:tensorflow:-----------------------------\n",
      "I0316 00:25:34.586491 140608677435200 run_squad.py:1388] -----------------------------\n",
      "INFO:tensorflow:0 Total Inference Time = 11.76 Inference Time W/O start up overhead = 1.89 Sentences processed = 16\n",
      "I0316 00:25:34.586577 140608677435200 run_squad.py:1389] 0 Total Inference Time = 11.76 Inference Time W/O start up overhead = 1.89 Sentences processed = 16\n",
      "INFO:tensorflow:0 Inference Performance = 8.4703 sentences/sec\n",
      "I0316 00:25:34.586667 140608677435200 run_squad.py:1392] 0 Inference Performance = 8.4703 sentences/sec\n",
      "INFO:tensorflow:-----------------------------\n",
      "I0316 00:25:34.586749 140608677435200 run_squad.py:1393] -----------------------------\n",
      "INFO:tensorflow:Writing predictions to: /workspace/bert/results/predictions.json\n",
      "I0316 00:25:34.586873 140608677435200 run_squad.py:792] Writing predictions to: /workspace/bert/results/predictions.json\n",
      "INFO:tensorflow:Writing nbest to: /workspace/bert/results/nbest_predictions.json\n",
      "I0316 00:25:34.586952 140608677435200 run_squad.py:793] Writing nbest to: /workspace/bert/results/nbest_predictions.json\n"
     ]
    }
   ],
   "source": [
    "# Ask BERT questions\n",
    "!python /workspace/bert/run_squad.py \\\n",
    "  --bert_config_file=$bert_config_file \\\n",
    "  --vocab_file=$vocab_file \\\n",
    "  --init_checkpoint=$init_checkpoint \\\n",
    "  --output_dir=$output_dir \\\n",
    "  --do_predict=$do_predict \\\n",
    "  --predict_file=$predict_file \\\n",
    "  --predict_batch_size=$predict_batch_size \\\n",
    "  --doc_stride=$doc_stride \\\n",
    "  --max_seq_length=$max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMnxQZb_WiUN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Id</th><th>Question</th><th>Answer</th></tr><tr><td>Q1</td><td>Who is Alan Turing?</td><td>father of theoretical computer science and artificial intelligence</td></tr><tr><td>Q2</td><td>When was Alan Turing born?</td><td>23 June 1912</td></tr><tr><td>Q3</td><td>Which laboratory Turing joined in 1948?</td><td>Max Newman's Computing Machine Laboratory at the Victoria University of Manchester</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results(predict_file, output_prediction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_colab_demo.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
